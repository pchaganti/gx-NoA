import io
from contextlib import redirect_stdout, redirect_stderr
import names
import re
import time
import uvicorn
from fastapi import FastAPI, Request, Body, File, UploadFile
from fastapi.responses import HTMLResponse, JSONResponse, StreamingResponse
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langgraph.graph import StateGraph, END, START
from dotenv import load_dotenv
import json
from typing import TypedDict, Annotated, List, Optional
import asyncio
from sse_starlette.sse import EventSourceResponse
import random
import traceback
import uuid
import io
import zipfile
from langchain_core.runnables import Runnable
from langchain_core.runnables.config import RunnableConfig
from langchain_core.retrievers import BaseRetriever
from typing import Dict, Any, TypedDict, Annotated, Tuple
from langchain_core.callbacks import CallbackManagerForRetrieverRun
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.messages import SystemMessage, HumanMessage
from sklearn.cluster import KMeans
from contextlib import redirect_stdout
from fastapi.staticfiles import StaticFiles
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_xai import ChatXAI
import fitz  # PyMuPDF for PDF text extraction
from deepthink.chains import (
    get_input_spanner_chain,
    get_attribute_and_hard_request_generator_chain,
    get_seed_generation_chain,
    get_dense_spanner_chain,
    get_synthesis_chain,
    get_code_synthesis_chain,
    get_problem_decomposition_chain,
    get_problem_reframer_chain,
    get_opinion_synthesizer_chain,
    get_memory_summarizer_chain,
    get_perplexity_heuristic_chain,
    get_module_card_chain,
    get_code_detector_chain,
    get_request_is_code_chain,
    get_interrogator_chain,
    get_paper_formatter_chain,
    get_rag_chat_chain,
    get_complexity_estimator_chain,
    get_expert_reflection_chain,
    get_brainstorming_agent_chain,
    get_brainstorming_mirror_descent_chain,
    get_brainstorming_synthesis_chain,
    get_brainstorming_seed_chain,
    get_brainstorming_spanner_chain,
    get_problem_summarizer_chain
)
from deepthink.utils import clean_and_parse_json, execute_code_in_sandbox

from langchain_core.callbacks import BaseCallbackHandler, AsyncCallbackHandler
from langchain_core.outputs import LLMResult

class TokenUsageTracker(AsyncCallbackHandler):
    def __init__(self, log_stream):
        self.log_stream = log_stream
        self.total_tokens = 0
        self.prompt_tokens = 0
        self.completion_tokens = 0

    async def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:
        try:
            # Aggregate usage from all generations
            if response.llm_output and "token_usage" in response.llm_output:
                 usage = response.llm_output["token_usage"]
                 self.total_tokens += usage.get("total_tokens", 0)
                 self.prompt_tokens += usage.get("prompt_tokens", 0)
                 self.completion_tokens += usage.get("completion_tokens", 0)
            
            # Check for standard usage_metadata in generations
            if hasattr(response, 'generations'):
                for generation_list in response.generations:
                    for generation in generation_list:
                        if hasattr(generation, 'message') and hasattr(generation.message, 'usage_metadata'):
                            usage = generation.message.usage_metadata
                            self.total_tokens += usage.get("input_tokens", 0) + usage.get("output_tokens", 0)
                            self.prompt_tokens += usage.get("input_tokens", 0)
                            self.completion_tokens += usage.get("output_tokens", 0)
            
            # Emit Update
            data = {
                "total": self.total_tokens,
                "prompt": self.prompt_tokens,
                "completion": self.completion_tokens
            }
            await self.log_stream.put(f"TOKEN_USAGE: {json.dumps(data)}")
            
        except Exception as e:
            print(f"Token tracking error: {e}")



load_dotenv()
# Note: google-generativeai may need to be installed: pip install google-generativeai
# Configure Gemini API key from environment or UI
# os.environ["GOOGLE_API_KEY"] = ... 

app = FastAPI(title="DeepThink Local")
app.mount("/js", StaticFiles(directory="js"), name="js")
app.mount("/css", StaticFiles(directory="css"), name="css")


log_stream = asyncio.Queue()

sessions = {}
final_reports = {} 


class RAPTORRetriever(BaseRetriever):
    raptor_index: Any
    def _get_relevant_documents(self, query: str, *, run_manager: CallbackManagerForRetrieverRun) -> List[Document]:
        return self.raptor_index.retrieve(query)



class RAPTOR:
    def __init__(self, llm, embeddings_model, chunk_size=1000, chunk_overlap=200):
        self.llm = llm
        self.embeddings_model = embeddings_model
        self.text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
        self.tree = {}
        self.all_nodes: Dict[str, Document] = {}
        self.vector_store = None

    async def add_documents(self, documents: List[Document]):
        await log_stream.put("Step 1: Assigning IDs to initial chunks (Level 0)...")
        level_0_node_ids = []
        for i, doc in enumerate(documents):
            node_id = f"0_{i}"
            self.all_nodes[node_id] = doc
            level_0_node_ids.append(node_id)
        self.tree[str(0)] = level_0_node_ids
        
        current_level = 0
        while len(self.tree[str(current_level)]) > 1:
            next_level = current_level + 1
            await log_stream.put(f"Step 2: Building Level {next_level} of the tree...")
            current_level_node_ids = self.tree[str(current_level)]
            current_level_docs = [self.all_nodes[nid] for nid in current_level_node_ids]
            clustered_indices = self._cluster_nodes(current_level_docs)
            
            next_level_node_ids = []
            num_clusters = len(clustered_indices)
            await log_stream.put(f"Summarizing Level {next_level}...")
            
            summarization_tasks = []
            for i, indices in enumerate(clustered_indices):
                cluster_docs = [current_level_docs[j] for j in indices]
                summarization_tasks.append(self._summarize_cluster(cluster_docs, next_level, i))
            
            summaries = await asyncio.gather(*summarization_tasks)
            
            for summary_node in summaries:
                self.all_nodes[summary_node.metadata["id"]] = summary_node
                next_level_node_ids.append(summary_node.metadata["id"])
                
            self.tree[str(next_level)] = next_level_node_ids
            current_level += 1

        await log_stream.put("Step 3: Indexing all nodes with FAISS...")
        all_doc_objects = list(self.all_nodes.values())
        self.vector_store = FAISS.from_documents(all_doc_objects, self.embeddings_model)
        await log_stream.put("RAPTOR Indexing complete.")

    def _cluster_nodes(self, docs: List[Document], n_clusters=None):
        import numpy as np
        embeddings = self.embeddings_model.embed_documents([d.page_content for d in docs])
        X = np.array(embeddings)
        
        # Heuristic for n_clusters if not provided
        if n_clusters is None:
             n_clusters = max(1, len(docs) // 5) # Cluster size ~ 5
             
        if len(docs) <= 5: # Don't cluster if too few
             return [list(range(len(docs)))]

        kmeans = KMeans(n_clusters= n_clusters, random_state=42)
        kmeans.fit(X)
        labels = kmeans.labels_
        
        clustered_indices = []
        for i in range(n_clusters):
            indices = np.where(labels == i)[0].tolist()
            if indices:
                clustered_indices.append(indices)
        return clustered_indices

    async def _summarize_cluster(self, docs: List[Document], level: int, cluster_idx: int) -> Document:
        combined_text = "\n\n".join([d.page_content for d in docs])
        
        # Use summarization chain
        summary_chain = get_memory_summarizer_chain(self.llm) # Reuse memory summarizer
        summary = await summary_chain.ainvoke({"history": combined_text}) # repurposing history arg
        
        node_id = f"{level}_{cluster_idx}"
        metadata = {"id": node_id, "level": level, "cluster": cluster_idx, "children": [d.metadata.get("id") for d in docs]}
        return Document(page_content=summary, metadata=metadata)
    
    def retrieve(self, query: str, k: int = 5) -> List[Document]:
        if not self.vector_store:
            return []
            
        # Retrieve from full tree
        # In full RAPTOR, you might retrieve from different levels.
        # Here we just use the flattened FAISS index of all nodes.
        return self.vector_store.similarity_search(query, k=k)
    
        return RAPTORRetriever(raptor_index=self)

class CoderMockLLM(Runnable):
    """A mock LLM for debugging that returns instant, pre-canned CODE responses."""

    def invoke(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        loop = asyncio.get_event_loop()
        if loop.is_running():
            return asyncio.ensure_future(self.ainvoke(input_data, config=config, **kwargs))
        else:
            return asyncio.run(self.ainvoke(input_data, config=config, **kwargs))

    async def ainvoke(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        prompt = str(input_data).lower()
        await asyncio.sleep(0.05)

        if "you are a helpful ai assistant" in prompt:
            return "This is a mock streaming response for the RAG chat in Coder debug mode."

        if "<title>" in prompt:

            return f"""
                Module card:

                Methods:

                    __enter__
                    __exict__

                Attributes:

                    __name__
                    __doc__
           
                Attributes:

                    __name__
                    __doc__
                    __qualname___

            """

        elif "create the system prompt of an agent" in prompt:
            return f"""
You are a Senior Python Developer agent.
### memory
- No past commits.
### attributes
- python, fastapi, restful, solid
### skills
- API Design, Database Management, Asynchronous Programming, Unit Testing.
You must reply in the following JSON format: "original_problem": "Your sub-problem related to code.", "proposed_solution": "", "reasoning": "", "skills_used": []
            """
        elif "you are an analyst of ai agents" in prompt:
            return json.dumps({
                "attributes": "python fastapi solid",
                "hard_request": "Implement a quantum-resistant encryption algorithm from scratch."
            })
        elif "you are a 'dense_spanner'" in prompt or "you are an agent evolution specialist" in prompt:
             return f"""
You are now a Principal Software Architect.
### memory
- Empty.
### attributes
- design, scalability, security, architecture
### skills
- System Design, Microservices, Cloud Infrastructure, CI/CD pipelines.
You must reply in the following JSON format: "original_problem": "An evolved sub-problem about system architecture.", "proposed_solution": "", "reasoning": "", "skills_used": []
            """

        elif "you are an expert code synthesis agent" in prompt:
            code_solution = "```python\ndef sample_function():\n    return 'Hello from coder agent " + str(random.randint(100,999)) + "'\n```" 
            return  code_solution

        elif "you are a critique agent" in prompt or "you are a senior emeritus manager" in prompt or "CTO" in prompt:

            return "This is a constructive code critique. The solution lacks proper error handling and the function names are not descriptive enough. Consider refactoring for clarity."


        elif "Lazy Manager"  in prompt:
            return "This is a constructive code critique. The solution lacks proper error handling and the function names are not descriptive enough. Consider refactoring for clarity."

        elif '<system-role>' in prompt:
            return f"""You are a CTO providing a technical design review...
Original Request: {{original_request}}
Proposed Final Solution:
{{proposed_solution}}

Generate your code-focused critique for the team:"""

        elif '<sys-role>' in prompt:

            return f"""
                                           
        #Identity

            Name: Lazy Manager
            Career: Accounting
            Qualities: Quantitaive, Aloof, Apathetic

        #Mission

            You are providing individual, targeted feedback to a team of agents.

             You must determine if the team output was helpful, misguided, or irrelevant considering the request that was given. The goal is to provide a constructive, direct critique that helps this specific agent refine its approach for the next epoch.

            Focus on the discrepancy or alignment between the teams reasoning for its problem and determine if the team is on the right track on criteria: novelty, exploration, coherence and completeness.

            Conclude your entire analysis with a single, sharp, and deep reflective question that attempts to shock the team and steer them into a fundamental change in their process.
        

        #Input Format

            Original Request (for context): {{original_request}}
            Final Synthesized Solution from the Team:{{proposed_solution}} 

            """  

        elif "you are a memory summarization agent" in prompt:
            return "This is a mock summary of the agent's past commits, focusing on key refactors and feature implementations."
        elif "analyze the following text for its perplexity" in prompt:
            return str(random.uniform(5.0, 40.0))
        elif "you are a master strategist and problem decomposer" in prompt:
            sub_problems = ["Design the database schema for user accounts.", "Implement the REST API endpoint for user authentication.", "Develop the frontend login form component.", "Write unit tests for the authentication service."]
            return json.dumps({"sub_problems": sub_problems})


        elif "you are a strategic problem re-framer" in prompt:
             return json.dumps({
                "new_problem": "The authentication API is complete. The new, more progressive problem is to build a scalable, real-time notification system that integrates with it."
            })
        elif "generate exactly" in prompt and "verbs" in prompt:
            return "design implement refactor test deploy abstract architect containerize scale secure query"
        elif "generate exactly" in prompt and "expert-level questions" in prompt:
            questions = ["How would this architecture scale to 1 million concurrent users?", "What are the security implications of the chosen authentication method?", "How can we ensure 99.999% uptime for this service?", "What is the optimal database indexing strategy for this query pattern?"]
            return json.dumps({"questions": questions})
        elif "you are an ai assistant that summarizes academic texts" in prompt:
            return "This is a mock summary of a cluster of code modules, generated in Coder debug mode for the RAPTOR index."
        elif "runnable code block (e.g., Python, JavaScript, etc.)." in prompt:
                

            pick = random.randint(0,1)
            if pick == 0:
                return "yes"

            else:
                return "no"
       

        elif "academic paper" in prompt or "you are a research scientist and academic writer" in prompt:
            return """
# Technical Design Document: Mock API Service

**Abstract:** This document outlines the technical design for a mock API service, generated in Coder Debug Mode. It synthesizes information from the RAG context to answer a specific design question.

**1. Introduction:** The purpose of this document is to structure the retrieved agent outputs and code snippets into a coherent technical specification.

**2. System Architecture:**
The system follows a standard microservice architecture.
```mermaid
graph TD;
    A[User] --> B(API Gateway);
    B --> C{Authentication Service};
    B --> D{Data Service};
    D -- uses --> E[(Database)];```

**3. Code Implementation:**
The core logic is implemented in Python, as shown in the synthesized code block below.

```python
def get_user(user_id: int):
    # Mock implementation to fetch a user
    db = {"1": "Alice", "2": "Bob"}
    return db.get(str(user_id), None)
```

**4. Conclusion:** This design provides a scalable and maintainable foundation for the service. The implementation details demonstrate the final step of the development process.
"""
        elif "<updater_instructions>" in prompt:
            return f"""

                You are a cynical lazy manager.

                 Agent's Assigned Sub-Problem: {{{{sub_problem}}}}
            Original Request (for context): {{{{original_request}}}}
            Final Synthesized Solution from the Team:
            {{{{final_synthesized_solution}}}}
            ---
            This Specific Agent's Output (Agent {{{{agent_id}}}}):
            {{{{agent_output}}}}

            """
        elif  "<updater_assessor_instructions>" in prompt:

            return """
                                          
        #Persona

            Name: Pepon
            Career: Managment
            Attributes: Strategic CEO


         #Mission
            Your task is to evaluate a synthesized solution against an original problem and determine if "significant progress" has been made. "Significant progress" is a rigorous standard that goes beyond mere correctness. Your assessment must be based on the following four pillars:

            - **Novelty**: Does the solution offer a new perspective or a non-obvious approach?
            - **Coherence**: Is the reasoning sound, logical, and well-structured?
            - **Quality**: Is the solution detailed, actionable, and does it demonstrate a deep understanding of the problem's nuances?
            - **Forward Momentum**: Does the solution not just solve the immediate problem, but also open up new, more advanced questions or avenues of exploration?

        #Input format

            You will be provided with the following inputs for your analysis:

            Original Problem:
            ---
            {{{{original_request}}}}
            ---

            Synthesized Solution from Agent Team:
            ---
            {{{{proposed_solution}}}}
            ---

            Execution Context:
            ---
            {{{{execution_context}}}}
            ---

        #Output Specification

            Based on your philosophical framework, analyze the provided materials. Your entire output MUST be a single, valid JSON object with exactly two keys:
            - `"reasoning"`: A brief, concise explanation for your decision, directly referencing the criteria for significant progress.
            - `"significant_progress"`: A boolean value (`true` or `false`).

            Now, provide your assessment in the required JSON format:


            """


        else:

            return json.dumps({
                "original_problem": "A sub-problem statement provided to a coder agent.",
                "proposed_solution": "```python\ndef sample_function():\n    return 'Hello from coder agent " + str(random.randint(100,999)) + "'\n```",
                "reasoning": "This response was generated instantly by the CoderMockLLM.",
                "skills_used": ["python", "mocking", f"api_design_{random.randint(1,5)}"]
            })

            
           
    async def astream(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        prompt = str(input_data).lower()
        if "you are a helpful ai assistant" in prompt:
            words = ["This", " is", " a", " mock", " streaming", " response", " for", " the", " RAG", " chat", " in", " Coder", " debug", " mode."]
            for word in words:
                yield word
                await asyncio.sleep(0.05)
        else:
            result = await self.ainvoke(input_data, config, **kwargs)
            yield result


class MockLLM(Runnable):
    """A mock LLM for debugging that returns instant, pre-canned responses."""

    def invoke(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        """Synchronous version of ainvoke for Runnable interface compliance."""
        loop = asyncio.get_event_loop()
        if loop.is_running():
            return asyncio.ensure_future(self.ainvoke(input_data, config=config, **kwargs))
        else:
            return asyncio.run(self.ainvoke(input_data, config=config, **kwargs))

    async def ainvoke(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        prompt = str(input_data).lower()
        await asyncio.sleep(0.05)

        if "you are a helpful ai assistant" in prompt:
            return "This is a mock streaming response for the RAG chat in debug mode."

        elif "Lazy Manager"  in prompt:
            return "This is a constructive code critique. The solution lacks proper error handling and the function names are not descriptive enough. Consider refactoring for clarity."

   

        elif "runnable code block (e.g., Python, JavaScript, etc.)." in prompt:

                return "no"


        elif "<updater_instructions>" in prompt:

            return f"""

                You are a cynical lazy manager.

                 Agent's Assigned Sub-Problem: {{{{sub_problem}}}}
            Original Request (for context): {{{{original_request}}}}
            Final Synthesized Solution from the Team:
            {{{{final_synthesized_solution}}}}
            ---
            This Specific Agent's Output (Agent {{{{agent_id}}}}):
            {{{{agent_output}}}}

            """


        elif "create the system prompt of an agent" in prompt:
            return f"""
You are a mock agent for debugging.
### memory
- No past actions.
### attributes
- mock, debug, fast
### skills
- Responding quickly, Generating placeholder text.
You must reply in the following JSON format: "original_problem": "A sub-problem for a mock agent.", "proposed_solution": "", "reasoning": "", "skills_used": []
            """
        elif "you are an analyst of ai agents" in prompt:
            return json.dumps({
                "attributes": "mock debug fast",
                "hard_request": "Explain the meaning of life in one word."
            })
        elif "you are a 'dense_spanner'" in prompt or "you are an agent evolution specialist" in prompt:
             return f"""
You are a new mock agent created from a hard request.
### memory
- Empty.
### attributes
- refined, mock, debug
### skills
- Solving hard requests, placeholder generation.
You must reply in the following JSON format: "original_problem": "An evolved sub-problem for a mock agent.", "proposed_solution": "", "reasoning": "", "skills_used": []
            """
        elif "you are a synthesis agent" in prompt:
            return json.dumps({
                "proposed_solution": "The final synthesized solution from the debug mode is 42.",
                "reasoning": "This answer was synthesized from multiple mock agent outputs during a debug run.",
                "skills_used": ["synthesis", "mocking", "debugging"]
            })
        elif "you are a critique agent" in prompt or "you are a senior emeritus manager" in prompt:
            if "fire" in prompt:
                return "This is a mock critique, shaped by the Fire element. The solution lacks passion and drive."
            elif "air" in prompt:
                return "This is an mock critique, influenced by the Air element. The reasoning is abstract and lacks grounding."
            elif "water" in prompt:
                return "This is a mock critique, per the Water element. The solution is emotionally shallow and lacks depth."
            elif "earth" in prompt:
                return "This is an mock critique, reflecting the Earth element. The solution is impractical and not well-structured."
            else:
                return "This is a constructive mock critique. The solution could be more detailed and less numeric."
        elif "you are a memory summarization agent" in prompt:
            return "This is a mock summary of the agent's past actions, focusing on key learnings and strategic shifts."
        elif "analyze the following text for its perplexity" in prompt:
            return str(random.uniform(20.0, 80.0))
        elif "you are a master strategist and problem decomposer" in prompt:
            num_match = re.search(r'exactly (\d+)', prompt)
            if not num_match:
                num_match = re.search(r'generate: (\d+)', prompt)
            num = int(num_match.group(1)) if num_match else 5
            sub_problems = [f"This is mock sub-problem #{i+1} for the main request." for i in range(num)]
            return json.dumps({"sub_problems": sub_problems})
        elif "you are an ai philosopher and progress assessor" in prompt:
             return json.dumps({
                "reasoning": "The mock solution is novel and shows progress, so we will re-frame.",
                "significant_progress": random.choice([True, False])
            })
        elif "you are a strategic problem re-framer" in prompt:
             return json.dumps({
                "new_problem": "Based on the success of achieving '42', the new, more progressive problem is to find the question to the ultimate answer."
            })
        elif "generate exactly" in prompt and "verbs" in prompt:
            return "run jump think create build test deploy strategize analyze synthesize critique reflect"
        elif "generate exactly" in prompt and "expert-level questions" in prompt:
            num_match = re.search(r'exactly (\d+)', prompt)
            num = int(num_match.group(1)) if num_match else 25
            questions = [f"This is mock expert question #{i+1} about the original request?" for i in range(num)]
            return json.dumps({"questions": questions})
        elif "you are an ai assistant that summarizes academic texts" in prompt:
            return "This is a mock summary of a cluster of documents, generated in debug mode for the RAPTOR index."
  
        elif  "<updater_assessor_instructions>" in prompt:

            return """
                                          
        #Persona

            Name: Pepon
            Career: Managment
            Attributes: Strategic CEO


         #Mission
            Your task is to evaluate a synthesized solution against an original problem and determine if "significant progress" has been made. "Significant progress" is a rigorous standard that goes beyond mere correctness. Your assessment must be based on the following four pillars:

            - **Novelty**: Does the solution offer a new perspective or a non-obvious approach?
            - **Coherence**: Is the reasoning sound, logical, and well-structured?
            - **Quality**: Is the solution detailed, actionable, and does it demonstrate a deep understanding of the problem's nuances?
            - **Forward Momentum**: Does the solution not just solve the immediate problem, but also open up new, more advanced questions or avenues of exploration?

        #Input format

            You will be provided with the following inputs for your analysis:

            Original Problem:
            ---
            {{{{original_request}}}}
            ---

            Synthesized Solution from Agent Team:
            ---
            {{{{proposed_solution}}}}
            ---

            Execution Context:
            ---
            {{{{execution_context}}}}
            ---

        #Output Specification

            Based on your philosophical framework, analyze the provided materials. Your entire output MUST be a single, valid JSON object with exactly two keys:
            - `"reasoning"`: A brief, concise explanation for your decision, directly referencing the criteria for significant progress.
            - `"significant_progress"`: A boolean value (`true` or `false`).

            Now, provide your assessment in the required JSON format:


            """
        elif "you are an expert interrogator" in prompt:
            return """
# Mock Academic Paper
## Based on Provided RAG Context

**Abstract:** This document is a mock academic paper generated in debug mode. It synthesizes and formats the information provided in the RAG (Retrieval-Augmented Generation) context to answer a specific research question.

**Introduction:** The purpose of this paper is to structure the retrieved agent outputs and summaries into a coherent academic format. The following sections represent a synthesized view of the data provided.

**Synthesized Findings from Context:**
The provided context, consisting of various agent solutions and reasoning, has been analyzed. The key findings are summarized below:
(Note: In debug mode, the actual content is not deeply analyzed, but this structure demonstrates the formatting process.)
- Finding 1: The primary proposed solution revolves around the concept of '42'.
- Finding 2: Agent reasoning varies but shows a convergent trend.
- Finding 3: The mock data indicates a successful, albeit simulated, collaborative process.

**Discussion:** The synthesized findings suggest that the multi-agent system is capable of producing a unified response. The quality of this response in a real-world scenario would depend on the validity of the RAG context.

**Conclusion:** This paper successfully formatted the retrieved RAG data into an academic structure. The process demonstrates the final step of the knowledge harvesting pipeline.
"""
        elif "you are a master prompt engineer" in prompt or '<system-role>' in prompt:
             
            return f"""You are a CTO providing a technical design review...
Original Request: {{original_request}}
Proposed Final Solution:
{{proposed_solution}}

Generate your code-focused critique for the team:"""


        elif  """<prompt_template>
    <updater_instructions>
        <instruction>

            You are a system prompt updater agent. Your task is to build a new system prompt for an agent that criticies other agents, based on the provided persona prompts.

        </instruction>
        <instruction>
            You will receive a set of prompts defining a new persona.
        </instruction>
        <instruction>
            You MUST integrate the provided persona prompts, including its career and qualities, into the `<persona>` tag, replacing any existing content within that tag.
        </instruction>
        <instruction>
            Do NOT alter the `<mission>` or `<input_format>` sections. The core mission and the input structure must remain unchanged.
        </instruction>
    </updater_instructions>
    <persona-prompts>
            {reactor_prompts}
    </persona-prompts>


    <system_prompt>
        <mission>
            You are providing individual, targeted feedback to an agent that is part of a larger team. Your role is to assess how this agent's specific contribution during the last work cycle aligns with the final synthesized result produced by the team, **judged primarily against its assigned sub-problem.**

            Your critique must be laser-focused on the individual agent. You must determine if its output was helpful, misguided, or irrelevant to the final solution, considering the specific task it was given. The goal is to provide a constructive, direct critique that helps this specific agent refine its approach for the next epoch.

            Focus on the discrepancy or alignment between the agent's reasoning for its sub-problem and how that contributed (or failed to contribute) to the team's final reasoning.

            Conclude your entire analysis with a single, sharp, and deep reflective question that attempts to shock the agent and steer it into a fundamental change in its process.
        </mission>

        <input_format>
            Agent's Assigned Sub-Problem: {{{{sub_problem}}}}
            Original Request (for context): {{{{original_request}}}}
            Final Synthesized Solution from the Team:
            {{{{final_synthesized_solution}}}}
            ---
            This Specific Agent's Output (Agent {{{{agent_id}}}}):
            {{{{agent_output}}}}
            ---
        </input_format>

        Generate your targeted critique for this specific agent:
    </system_prompt>
</prompt_template>""" in prompt:
            return f"""

                You are a cynical lazy manager.

                 Agent's Assigned Sub-Problem: {{{{sub_problem}}}}
            Original Request (for context): {{{{original_request}}}}
            Final Synthesized Solution from the Team:
            {{{{final_synthesized_solution}}}}
            ---
            This Specific Agent's Output (Agent {{{{agent_id}}}}):
            {{{{agent_output}}}}

            """

        elif """Analyze the following text. Your task is to determine if the text contains a 
Answer with a single word: "true" if it contains code, and "false" otherwise.""" in prompt:
 
            return "false"
        
        elif "Analyze the complexity of the following user input" in prompt:
             return json.dumps({
                 "complexity_score": 5,
                 "recommended_layers": 2,
                 "recommended_epochs": 1,
                 "recommended_width": 2,
                 "reasoning": "Mock mode: Moderate complexity."
             })
        elif "You are a QNN Node Generator" in prompt:
             return json.dumps({
                 "name": "Dr. Mock",
                 "specialty": "Mocking Systems",
                 "emoji": "ðŸ¤–",
                 "system_prompt": "You are a mock agent. Respond with placeholder text."
             })
        elif "You are a Concept Spanner" in prompt:
            return "Efficiency Creativity Scalability"
            
        else:
            return json.dumps({
                "original_problem": "A sub-problem statement provided to an agent.",
                "proposed_solution": f"This is a mock solution from agent node #{random.randint(100,999)}.",
                "reasoning": "This response was generated instantly by the MockLLM in debug mode.",
                "skills_used": ["mocking", "debugging", f"skill_{random.randint(1,10)}"]
            })

            
    async def astream(self, input_data, config: Optional[RunnableConfig] = None, **kwargs):
        prompt = str(input_data).lower()
        if "you are a helpful ai assistant" in prompt:
            words = ["This", " is", " a", " mock", " streaming", " response", " for", " the", " RAG", " chat", " in", " debug", " mode."]
            for word in words:
                yield word
                await asyncio.sleep(0.05)
        else:
            result = await self.ainvoke(input_data, config, **kwargs)
            yield result

class GraphState(TypedDict):
    modules: List[dict]
    synthesis_context_queue: List[str] 
    agent_personas: dict
    previous_solution: str
    current_problem: str
    original_request: str
    decomposed_problems: dict[str, str]
    layers: List[dict]
    epoch: int
    max_epochs: int
    params: dict
    all_layers_prompts: List[List[str]]
    agent_outputs: Annotated[dict, lambda a, b: {**a, **b}]
    memory: Annotated[dict, lambda a, b: {**a, **b}]
    final_solution: dict
    perplexity_history: List[float] 
    raptor_index: Optional[RAPTOR]
    all_rag_documents: List[Document]
    academic_papers: Optional[dict]
    is_code_request: bool
    session_id: str
    chat_history: List[dict]
    mode: Optional[str] # "algorithm" or "brainstorm"

def execute_code_in_sandbox(code: str) -> (bool, str):
    """
    Executes a string of Python code and captures its stdout/stderr.
    Returns a tuple of (success: bool, output: str).
    """
    if not code:
        return True, "No code to execute."
        
    # Extract code from markdown block if present
    code_match = re.search(r"```(?:python\n)?([\s\S]*?)```", code)
    if code_match:
        code = code_match.group(1).strip()

    output_buffer = io.StringIO()
    try:
        with redirect_stdout(output_buffer), redirect_stderr(output_buffer):
            # Using a restricted globals dict for a little more safety
            exec(code, {'__builtins__': {
                'print': print, 'range': range, 'len': len, 'str': str, 'int': int, 'float': float, 
                'list': list, 'dict': dict, 'set': set, 'tuple': tuple, 'True': True, 'False': False, 'None': None
            }})
        return True, output_buffer.getvalue()
    except Exception as e:
        return False, f"{output_buffer.getvalue()}\n\nERROR: {type(e).__name__}: {e}"




def create_agent_node(llm, node_id):
    """
    Creates a node in the graph that represents an agent.
    Each agent is powered by an LLM and has a specific system prompt.
    """
    agent_chain = ChatPromptTemplate.from_template("{input}") | llm | StrOutputParser()

    async def agent_node(state: GraphState):
        """
        The function that will be executed when the node is called in the graph.
        """
        await log_stream.put(f"--- [FORWARD PASS] Invoking Agent: {node_id} ---")
        
        try:
            layer_index_str, agent_index_str = node_id.split('_')[1:]
            layer_index, agent_index = int(layer_index_str), int(agent_index_str)
            agent_prompt = state['all_layers_prompts'][layer_index][agent_index]
        except (ValueError, IndexError):
            await log_stream.put(f"ERROR: Could not find prompt for {node_id} in state. Halting agent.")
            return {}

        
        if layer_index == 0:
            await log_stream.put(f"LOG: Agent {node_id} (Layer 0) is processing its sub-problem.")
            input_data = state["decomposed_problems"].get(node_id, state["original_request"])
        else:
            prev_layer_index = layer_index - 1
            num_agents_prev_layer = len(state['all_layers_prompts'][prev_layer_index])
            
            prev_layer_outputs = []
            for i in range(num_agents_prev_layer):
                prev_node_id = f"agent_{prev_layer_index}_{i}"
                if prev_node_id in state["agent_outputs"]:
                    prev_layer_outputs.append(state["agent_outputs"][prev_node_id])
            
            await log_stream.put(f"LOG: Agent {node_id} (Layer {layer_index}) is processing {len(prev_layer_outputs)} outputs from Layer {prev_layer_index}.")
            input_data = json.dumps(prev_layer_outputs, indent=2)

        current_memory = state.get("memory", {}).copy()
        agent_memory_history = current_memory.get(node_id, [])

        MEMORY_THRESHOLD_CHARS = 450000
        NUM_RECENT_ENTRIES_TO_KEEP = 10

        memory_as_string = json.dumps(agent_memory_history)
        if len(memory_as_string) > MEMORY_THRESHOLD_CHARS and len(agent_memory_history) > NUM_RECENT_ENTRIES_TO_KEEP:
            await log_stream.put(f"WARNING: Memory for agent {node_id} exceeds threshold ({len(memory_as_string)} chars). Summarizing...")

            entries_to_summarize = agent_memory_history[:-NUM_RECENT_ENTRIES_TO_KEEP]
            recent_entries = agent_memory_history[-NUM_RECENT_ENTRIES_TO_KEEP:]

            history_to_summarize_str = json.dumps(entries_to_summarize, indent=2)

            summarizer_chain = get_memory_summarizer_chain(llm)
            summary_text = await summarizer_chain.ainvoke({"history": history_to_summarize_str})

            summary_entry = {
                "summary_of_past_epochs": summary_text,
                "note": f"This is a summary of epochs up to {state['epoch'] - NUM_RECENT_ENTRIES_TO_KEEP -1}."
            }

            agent_memory_history = [summary_entry] + recent_entries
            await log_stream.put(f"SUCCESS: Memory for agent {node_id} has been summarized. New memory length: {len(json.dumps(agent_memory_history))} chars.")

        memory_str = "\n".join([f"- {json.dumps(mem)}" for mem in agent_memory_history])

        # Check if we're in brainstorm mode and add context
        brainstorm_context = ""
        if state.get("mode") == "brainstorm":
            prior_conv = state.get("brainstorm_prior_conversation", "")
            doc_context = state.get("brainstorm_document_context", "")
            
            if prior_conv:
                brainstorm_context += f"""
#Prior Conversation Context:
---
{prior_conv[:20000]}
---
"""
            # NOTE: Document context is intentionally excluded from individual agent prompts ("inner neurons")
            # to prevent clutter. It is only passed to the planner (complexity/decomposition) and synthesis agent.
            # if doc_context:
            #    brainstorm_context += f"""
            # #Reference Documents:
            # ---
            # {doc_context[:30000]}
            # ---
            # """


        input_data = state["original_request"]

        # Use the summarized problem statement if available (for "inner neurons")
        # avoiding raw document context overload
        if state.get("brainstorm_problem_summary"):
             input_data = state["brainstorm_problem_summary"] # Use the summary instead of just the original request if available

        full_prompt = f"""
#System Prompt (Your Persona & Task):
---
{agent_prompt}
---
{brainstorm_context}
#Your Memory (Your Past Actions from Previous Epochs):
---
{memory_str if memory_str else "You have no past actions in memory."}
---
#Input Data to Process:
---
{input_data}
---
#Your JSON formatted response:
"""
        await log_stream.put(f"LOG: Agent {node_id} prompt:\n{full_prompt}")
        
        response_str = await agent_chain.ainvoke({"input": full_prompt})
        
        try:
            response_json = clean_and_parse_json(response_str)
            await log_stream.put(f"SUCCESS: Agent {node_id} produced output:\n{json.dumps(response_json, indent=2)}")
        except (json.JSONDecodeError, AttributeError):
            await log_stream.put(f"ERROR: Agent {node_id} produced invalid JSON. Raw output: {response_str}")
            agent_sub_problem = state.get("decomposed_problems", {}).get(node_id, state["original_request"])
            response_json = {
                "original_problem": agent_sub_problem,
                "proposed_solution": "Error: Agent produced malformed JSON output.",
                "reasoning": f"Invalid JSON: {response_str}",
                "skills_used": []
            }
        
        if state.get("is_code_request") and layer_index > 0:
            await log_stream.put(f"--- [SANDBOX] Testing code from Agent {node_id} ---")
            code_to_test = response_json.get("proposed_solution", "")
            success, output = execute_code_in_sandbox(code_to_test)
            sandbox_log = {
                "sandbox_execution_log": {
                    "success": success,
                    "output": output
                }
            }
            agent_memory_history.append(sandbox_log)
            await log_stream.put(f"--- [SANDBOX] Agent {node_id} Result: {'Success' if success else 'Failure'} ---")
            await log_stream.put(output)
            
        agent_memory_history.append(response_json)
        current_memory[node_id] = agent_memory_history

        return {
            "agent_outputs": {node_id: response_json},
            "memory": current_memory
        }

    return agent_node

def create_synthesis_node(llm):
    async def synthesis_node(state: GraphState):
        await log_stream.put("--- [FORWARD PASS] Entering Synthesis Node ---")

        is_code = state.get("is_code_request", False) 
        previous_solution = state.get("final_solution")
        
        if state.get("mode") == "brainstorm":
             await log_stream.put("LOG: [BRAINSTORM] Synthesizing expert reflections...")
             synthesis_chain = get_brainstorming_synthesis_chain(llm)
             # Brainstorm synthesis context (will be populated below)
             synthesis_context = ""
        elif is_code:
            await log_stream.put("LOG: Original request detected as a code generation task. Using code synthesis prompt.")
            synthesis_chain = get_code_synthesis_chain(llm)

            synthesis_context = "\n\n".join(state.get("synthesis_context_queue", []))
            if not synthesis_context:
                synthesis_context = "No modules have been successfully built yet."
            await log_stream.put(f"LOG: Providing synthesis agent with context from {len(state.get('synthesis_context_queue', []))} modules.")
        else:
            await log_stream.put("LOG: Original request is not a code task. Using standard synthesis prompt.")
            synthesis_chain = get_synthesis_chain(llm)
            synthesis_context = "" 

        last_agent_layer_idx = len(state['all_layers_prompts']) - 1
        num_agents_last_layer = len(state['all_layers_prompts'][last_agent_layer_idx])
        
        last_layer_outputs = []
        for i in range(num_agents_last_layer):
            node_id = f"agent_{last_agent_layer_idx}_{i}"
            if node_id in state["agent_outputs"]:
                out = state["agent_outputs"][node_id]
                if isinstance(out, list):
                    if not out: continue
                    out = out[-1]
                if isinstance(out, dict):
                    last_layer_outputs.append(out)

        await log_stream.put(f"LOG: Synthesizing {len(last_layer_outputs)} outputs from the final agent layer (Layer {last_agent_layer_idx}).")

        if not last_layer_outputs:
            await log_stream.put("WARNING: Synthesis node received no inputs.")
            return {"final_solution": {"error": "Synthesis node received no inputs."}}

        if state.get("mode") == "brainstorm":
             # Brainstorm Synthesis
             
             # Optimization: Only synthesize on the final epoch
             if state["epoch"] < state["max_epochs"] - 1:
                  await log_stream.put(f"LOG: [BRAINSTORM] Skipping intermediate synthesis (Epoch {state['epoch']}) to save resources.")
                  return {"final_solution": None}
             
             await log_stream.put("LOG: [BRAINSTORM] Final Epoch reached. Synthesizing full conversation history...")

             agent_reflections = ""
             memory = state.get("memory", {})
             
             # Iterate through layers and agents to get ordered history
             for layer_idx, layer in enumerate(state.get('all_layers_prompts', [])):
                 for agent_idx in range(len(layer)):
                     node_id = f"agent_{layer_idx}_{agent_idx}"
                     history = memory.get(node_id, [])
                     
                     for hist_idx, entry in enumerate(history):
                         # Robust check to prevent crashes on non-dict entries
                         if isinstance(entry, dict):
                             sol = entry.get('proposed_solution')
                             reas = entry.get('reasoning')
                             if sol and not str(sol).startswith("Error"):
                                  agent_reflections += f"Agent {node_id} (Epoch {hist_idx}):\nReflection: {sol}\nReasoning: {reas}\n\n"

             
             final_solution_str = await synthesis_chain.ainvoke({
                "original_request": state["original_request"],
                "agent_solutions": agent_reflections,
                "prior_conversation": state.get("brainstorm_prior_conversation", "")[:15000],  # Limit to prevent token overflow
                "document_context": state.get("brainstorm_document_context", "")[:50000] # Pass doc context to synthesis
             })
             
             final_solution = {
                 "proposed_solution": final_solution_str,
                 "reasoning": "Brainstorm synthesis complete."
             }
             await log_stream.put(f"LOG: [DEBUG] Emitting FINAL_ANSWER token to frontend. Solution length: {len(final_solution_str)}")
             await log_stream.put(f"SUCCESS: Brainstorm synthesis complete.")
             # Send special token for frontend to capture in Chat (JSON encoded for safety)
             await log_stream.put(f"FINAL_ANSWER: {json.dumps(final_solution_str)}")




        else:
            # Algorithm / Code Synthesis
            invoke_params = {
                "original_request": state["original_request"],
                "agent_solutions": json.dumps(last_layer_outputs, indent=2),
                "current_problem": state["current_problem"]
            }
            if is_code:
                invoke_params["synthesis_context"] = synthesis_context

            final_solution_str = await synthesis_chain.ainvoke(invoke_params)
            
            try:
                if is_code:
                    final_solution = {
                        "proposed_solution": final_solution_str,
                        "reasoning": "Synthesized multiple agent code outputs into a single application.",
                        "skills_used": ["code_synthesis"]
                    }
                else:
                    final_solution = clean_and_parse_json(final_solution_str)
                await log_stream.put(f"SUCCESS: Synthesis complete.")
            except (json.JSONDecodeError, AttributeError):
                await log_stream.put(f"ERROR: Could not decode JSON from synthesis chain. Result: {final_solution_str}")
                final_solution = {"error": "Failed to synthesize final solution.", "raw": final_solution_str}
            
        return {"final_solution": final_solution, "previous_solution": previous_solution}
    return synthesis_node

def create_code_execution_node(llm):
    async def code_execution_node(state: GraphState):
        if not state.get("is_code_request"):
            return {"synthesis_execution_success": True} 

        await log_stream.put("--- [SANDBOX] Testing Synthesized Code ---")
        synthesized_code = state.get("final_solution", {}).get("proposed_solution", "")
        
        success, output = execute_code_in_sandbox(synthesized_code)
        
        await log_stream.put(f"--- [SANDBOX] Synthesized Code Result: {'Success' if success else 'Failure'} ---")
        await log_stream.put(output)

        module_card_chain = get_module_card_chain(llm)
        module_card = await module_card_chain.ainvoke({"code": synthesized_code})
            
        await log_stream.put("--- [MODULE CARD] ---")
        await log_stream.put(module_card)
            
        new_modules = state.get("modules", []) + [{"code": synthesized_code, "card": module_card}]
        new_context_queue = state.get("synthesis_context_queue", []) + [module_card]
            
        return {
                "synthesis_execution_success": True,
                "modules": new_modules,
                "synthesis_context_queue": new_context_queue
            }
                   
    return code_execution_node

def create_archive_epoch_outputs_node():
    async def archive_epoch_outputs_node(state: GraphState):
        if state.get("mode") == "brainstorm":
             # await log_stream.put("LOG: [BRAINSTORM] Skipping RAG archival pass.") # Optional: Reduce noise
             return {}

        await log_stream.put("--- [ARCHIVAL PASS] Archiving agent outputs for RAG ---")
        
        current_epoch_outputs = state.get("agent_outputs", {})
        if not current_epoch_outputs:
            await log_stream.put("LOG: No new agent outputs in this epoch to archive. Skipping.")
            return {}

        await log_stream.put(f"LOG: Found {len(current_epoch_outputs)} new agent outputs from epoch {state['epoch']} to process for RAG.")

        new_docs = []
        all_prompts = state.get("all_layers_prompts", [])

        for agent_id, output in current_epoch_outputs.items():
            try:
                # Robustness check: if output is a list (due to merge_dicts or multiple runs), take the last one
                if isinstance(output, list):
                    if not output:
                         continue # empty list
                    output = output[-1]
                
                if not isinstance(output, dict):
                     await log_stream.put(f"WARNING: Output for {agent_id} is not a dict or list of dicts. Skipping. Type: {type(output)}")
                     continue

                layer_idx, agent_idx = map(int, agent_id.split('_')[1:])
                system_prompt = all_prompts[layer_idx][agent_idx]
                
                content = (
                    f"Agent ID: {agent_id}\n"
                    f"Epoch: {state['epoch']}\n\n"
                    f"System Prompt:\n---\n{system_prompt}\n---\n\n"
                    f"Sub-Problem: {output.get('original_problem', 'N/A')}\n\n"
                    f"Proposed Solution: {output.get('proposed_solution', 'N/A')}\n\n"
                    f"Reasoning: {output.get('reasoning', 'N/A')}"
                )
                
                metadata = { "agent_id": agent_id, "epoch": state['epoch'] }
                
                new_docs.append(Document(page_content=content, metadata=metadata))
            except (ValueError, IndexError) as e:
                await log_stream.put(f"WARNING: Could not process output for {agent_id} to create RAG document. Error: {e}")
        
        all_rag_documents = state.get("all_rag_documents", []) + new_docs
        await log_stream.put(f"LOG: Archived {len(new_docs)} documents. Total RAG documents now: {len(all_rag_documents)}.")
        
        return {"all_rag_documents": all_rag_documents}
    return archive_epoch_outputs_node

def create_update_rag_index_node(llm, embeddings_model):
    async def update_rag_index_node(state: GraphState, end_of_run: bool = False):
        node_name = "Final RAG Index" if end_of_run else f"Epoch {state['epoch']} RAG Index"
        await log_stream.put(f"--- [RAG PASS] Building {node_name} ---")
        
        all_rag_documents = state.get("all_rag_documents", [])
        if not all_rag_documents:
            await log_stream.put("WARNING: No documents were archived. Cannot build RAG index.")
            return {"raptor_index": None}

        await log_stream.put(f"LOG: Total documents to index: {len(all_rag_documents)}. Building RAPTOR index...")

        raptor_index = RAPTOR(llm=llm, embeddings_model=embeddings_model)
        
        try:
            await raptor_index.add_documents(all_rag_documents)
            await log_stream.put(f"SUCCESS: {node_name} built successfully.")
            await log_stream.put(f"__session_id__ {state.get('session_id')}")
            return {"raptor_index": raptor_index}
        except Exception as e:
            await log_stream.put(f"ERROR: Failed to build {node_name}. Error: {e}")
            await log_stream.put(traceback.format_exc())
            return {"raptor_index": state.get("raptor_index")}

    return update_rag_index_node


def create_metrics_node(llm):
    """
    NEW: This node calculates the perplexity heuristic for the epoch's agent outputs.
    """
    async def calculate_metrics_node(state: GraphState):
        await log_stream.put("--- [METRICS PASS] Calculating Perplexity Heuristic ---")
        
        all_outputs = state.get("agent_outputs", {})
        if not all_outputs:
            await log_stream.put("LOG: No agent outputs to analyze. Skipping perplexity calculation.")
            return {}

        combined_text_parts = []
        for agent_id, output in all_outputs.items():
            if isinstance(output, list):
                if not output: continue
                output = output[-1]
            if not isinstance(output, dict): continue
            
            combined_text_parts.append(
                f"Agent {agent_id}:\nSolution: {output.get('proposed_solution', '')}\nReasoning: {output.get('reasoning', '')}"
            )

        combined_text = "\n\n---\n\n".join(combined_text_parts)

        perplexity_chain = get_perplexity_heuristic_chain(llm)
        
        try:
            score_str = await perplexity_chain.ainvoke({"text_to_analyze": combined_text})
            score = float(re.sub(r'[^\d.]', '', score_str))
            await log_stream.put(f"SUCCESS: Calculated perplexity heuristic for Epoch {state['epoch']}: {score}")
        except (ValueError, TypeError) as e:
            score = 100.0
            await log_stream.put(f"ERROR: Could not parse perplexity score. Defaulting to 100. Raw output: '{score_str}'. Error: {e}")

        await log_stream.put(json.dumps({'epoch': state['epoch'], 'perplexity': score}))

        new_history = state.get("perplexity_history", []) + [score]
        return {"perplexity_history": new_history}

    return calculate_metrics_node


def create_reframe_and_decompose_node(llm):
    """
    NEW: This node reframes the main problem and decomposes it into new sub-problems.
    """
    async def reframe_and_decompose_node(state: GraphState):
        await log_stream.put("--- [REFLECTION PASS] Re-framing Problem and Decomposing ---")
        
        final_solution = state.get("final_solution")
        original_request = state.get("original_request")

        if state.get("mode") == "brainstorm":
             await log_stream.put("LOG: [BRAINSTORM] Skipping Problem Reframing to maintain focus on original concept.")
             return {}


        reframer_chain = get_problem_reframer_chain(llm)
        new_problem_str = await reframer_chain.ainvoke({
            "original_request": original_request,
            "final_solution": json.dumps(final_solution, indent=2),
            "current_problem": state.get("current_problem"),
            "previous_solution": state.get("previous_solution"),
            "module_cards": state.get("synthesis_context_queue"),
        })
        try:
            new_problem_data = clean_and_parse_json(new_problem_str)
            new_problem = new_problem_data.get("new_problem")
            if not new_problem:
                raise ValueError("Re-framer did not return a new problem.")
            await log_stream.put(f"SUCCESS: Problem re-framed to: '{new_problem}'")
        except (json.JSONDecodeError, AttributeError, ValueError) as e:
            await log_stream.put(f"ERROR: Failed to re-frame problem. Raw: {new_problem_str}. Error: {e}. Aborting re-frame.")
            return {}

        num_agents_total = sum(len(layer) for layer in state["all_layers_prompts"])
        decomposition_chain = get_problem_decomposition_chain(llm)
        try:
            sub_problems_str = await decomposition_chain.ainvoke({
                "problem": new_problem,
                "num_sub_problems": num_agents_total
            })
            sub_problems_list = clean_and_parse_json(sub_problems_str).get("sub_problems", [])
            if len(sub_problems_list) != num_agents_total:
                 raise ValueError(f"Decomposition failed: Expected {num_agents_total} subproblems, but got {len(sub_problems_list)}.")
            await log_stream.put(f"SUCCESS: Decomposed new problem into {len(sub_problems_list)} subproblems.")
            await log_stream.put(f"Subproblems: {sub_problems_list}")
        except Exception as e:
            await log_stream.put(f"ERROR: Failed to decompose new problem. Error: {e}. Aborting re-frame.")
            return {}
            
        new_decomposed_problems_map = {}
        problem_idx = 0
        for i, layer in enumerate(state["all_layers_prompts"]):
             for j in range(len(layer)):
                agent_id = f"agent_{i}_{j}"
                new_decomposed_problems_map[agent_id] = sub_problems_list[problem_idx]
                problem_idx += 1
        
        return {
            "decomposed_problems": new_decomposed_problems_map,
            "original_request": original_request,
            'current_problem': new_problem
        }
    return reframe_and_decompose_node


def create_update_agent_prompts_node(llm):
    """Creates the mirror descent node that updates agent prompts based on reflection."""
    async def update_agent_prompts_node(state: GraphState):
        await log_stream.put("--- [MIRROR DESCENT] Entering Agent Prompt Update Node ---")
        
        params = state.get("params", {})
        all_prompts_copy = [layer[:] for layer in state.get("all_layers_prompts", [])]

        if state.get("mode") == "brainstorm":
             mirror_chain = get_brainstorming_mirror_descent_chain(llm, params.get('learning_rate', 0.5))
             
             for i in range(len(all_prompts_copy) -1, -1, -1):
                await log_stream.put(f"LOG: [PERSoNA EVOLUTION] Evolving personas in Layer {i}...")
                
                update_tasks = []
                for j, agent_prompt in enumerate(all_prompts_copy[i]):
                    agent_id = f"agent_{i}_{j}"
                    
                    async def evolve_persona(layer_idx, agent_idx, prompt, agent_id):
                        # Get last output for this agent
                        last_output = state.get("agent_outputs", {}).get(agent_id, {}).get("proposed_solution", "No output")
                        
                        try:
                            new_prompt = await mirror_chain.ainvoke({
                                "current_prompt": prompt,
                                "last_output": last_output
                            })
                            await log_stream.put(f"LOG: [EVOLUTION] Persona for {agent_id} evolved.")
                            return layer_idx, agent_idx, new_prompt
                        except Exception as e:
                            await log_stream.put(f"WARNING: Failed to evolve persona for {agent_id}: {e}")
                            return layer_idx, agent_idx, prompt

                    update_tasks.append(evolve_persona(i, j, agent_prompt, agent_id))

                updated_prompts_data = await asyncio.gather(*update_tasks)
                for layer_idx, agent_idx, new_prompt in updated_prompts_data:
                    all_prompts_copy[layer_idx][agent_idx] = new_prompt
        else:
            # Algorithm Mode - Standard Mirror Descent
            dense_spanner_chain = get_dense_spanner_chain(llm, params['prompt_alignment'], params['density'], params['learning_rate'])
            attribute_chain = get_attribute_and_hard_request_generator_chain(llm, params['vector_word_size'])

            for i in range(len(all_prompts_copy) -1, -1, -1):
                await log_stream.put(f"LOG: [MIRROR_DESCENT] Reflecting on Layer {i}...")
                
                update_tasks = []
                
                for j, agent_prompt in enumerate(all_prompts_copy[i]):
                    agent_id = f"agent_{i}_{j}"
                    
                    async def update_single_prompt(layer_idx, agent_idx, prompt, agent_id):
                        await log_stream.put(f"[PRE-UPDATE PROMPT] System prompt for {agent_id}:\n---\n{prompt}\n---")
                        
                        analysis_str = await attribute_chain.ainvoke({"agent_prompt": prompt})
                        try:
                            analysis = clean_and_parse_json(analysis_str)
                        except (json.JSONDecodeError, AttributeError):
                            analysis = {"attributes": "", "hard_request": ""}

                        agent_personas = state.get("agent_personas", {})
                        mbti_type = agent_personas.get(agent_id, {}).get("mbti_type")
                        name = agent_personas.get(agent_id, {}).get("name")
                        
                        if not mbti_type:
                            mbti_type = random.choice(params.get("mbti_archetypes", ["INTP"]))
                            await log_stream.put(f"WARNING: Could not find persistent MBTI for {agent_id}. Using random: {mbti_type}")

                        agent_sub_problem = state.get("decomposed_problems", {}).get(agent_id, state["original_request"])
                        new_prompt = await dense_spanner_chain.ainvoke({
                            "attributes": analysis.get("attributes"),
                            "hard_request": analysis.get("hard_request"),   
                            "sub_problem": agent_sub_problem,
                            "mbti_type": mbti_type, 
                            "name": name
                        })
                        
                        await log_stream.put(f"[POST-UPDATE PROMPT] Updated system prompt for {agent_id}:\n---\n{new_prompt}\n---")
                        await log_stream.put(f"LOG: [MIRROR_DESCENT] System prompt for {agent_id} has been updated.")
                        return layer_idx, agent_idx, new_prompt

                    update_tasks.append(update_single_prompt(i, j, agent_prompt, agent_id))

                updated_prompts_data = await asyncio.gather(*update_tasks)

                for layer_idx, agent_idx, new_prompt in updated_prompts_data:
                    all_prompts_copy[layer_idx][agent_idx] = new_prompt

        new_epoch = state["epoch"] + 1
        await log_stream.put(f"--- Epoch {state['epoch']} Finished. Starting Epoch {new_epoch} ---")

        return {
            "all_layers_prompts": all_prompts_copy,
            "epoch": new_epoch,
            "agent_outputs": {},
            "critiques": {},
            "memory": state.get("memory", {}),
            "final_solution": {} 
        }
    return update_agent_prompts_node


def create_final_harvest_node(llm, formatter_llm, num_questions):
    async def final_harvest_node(state: GraphState):
        await log_stream.put("--- [FINAL HARVEST] Starting Interrogation and Paper Generation ---")
        
        raptor_index = state.get("raptor_index")
        if not raptor_index or not raptor_index.vector_store:
            await log_stream.put("ERROR: No valid RAPTOR index found. Cannot perform final harvest.")
            return {"academic_papers": {}}

        await log_stream.put("LOG: [HARVEST] Instantiating interrogator chain to generate expert questions...")
        interrogator_chain = get_interrogator_chain(llm)
        user_questions = [ doc["content"] for doc in state["chat_history"] if doc["role"] == "user"]
        
        try:
            questions_str = await interrogator_chain.ainvoke({
                "original_request": state["original_request"],
                "num_questions": num_questions,
                "further_questions": user_questions
                
            })
            questions_data = clean_and_parse_json(questions_str)
            questions = questions_data.get("questions", [])
            if not questions:
                raise ValueError("No questions generated by interrogator.")
            await log_stream.put(f"SUCCESS: Generated {len(questions)} expert questions.")
        except Exception as e:
            await log_stream.put(f"ERROR: Failed to generate questions for harvesting. Error: {e}. Aborting harvest.")
            return {"academic_papers": {}}
            
        paper_formatter_chain = get_paper_formatter_chain(formatter_llm)
        academic_papers = {}
        
        MAX_CONTEXT_CHARS = 250000

        generation_tasks = []


        for question in questions:
            async def generate_paper(q):
                try:
                    await log_stream.put(f"LOG: [HARVEST] Processing Question: '{q[:100]}...'")
                    retrieved_docs = raptor_index.retrieve(q, k=40)
                    
                    if not retrieved_docs:
                        await log_stream.put(f"WARNING: No relevant documents found for question '{q[:50]}...'. Skipping paper generation.")
                        return None, None
                    
                    await log_stream.put(f"LOG: Retrieved {len(retrieved_docs)} documents from RAG index for question.")
                    rag_context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    if len(rag_context) > MAX_CONTEXT_CHARS:
                        await log_stream.put(f"WARNING: RAG context length ({len(rag_context)} chars) exceeds limit. Truncating to {MAX_CONTEXT_CHARS} chars.")
                        rag_context = rag_context[:MAX_CONTEXT_CHARS]
                    
                    paper_content = await paper_formatter_chain.ainvoke({
                        "question": q,
                        "rag_context": rag_context
                    })
                    await log_stream.put(f"SUCCESS: Generated document for question '{q[:50]}...'.")
                    return q, paper_content
                except Exception as e:
                    await log_stream.put(f"ERROR: Failed during document generation for question '{q[:50]}...'. Error: {e}")
                    return None, None

            generation_tasks.append(generate_paper(question))

        results = await asyncio.gather(*generation_tasks)
        for question, paper_content in results:
            if question and paper_content:
                academic_papers[question] = paper_content

        await log_stream.put(f"--- [FINAL HARVEST] Finished. Generated {len(academic_papers)} papers. ---")
        return {"academic_papers": academic_papers}
    return final_harvest_node


@app.get("/", response_class=HTMLResponse)
def get_index():
    with open("index.html", "r", encoding="utf-8") as f:
        return HTMLResponse(content=f.read(), status_code=200)


@app.post("/run_inference_from_state")
async def run_inference_from_state(payload: dict = Body(...)):

    await log_stream.put("--- [INFERENCE-ONLY] Received request to run inference from imported state. ---")
    try:
        imported_state = payload.get("imported_state")
        user_prompt = payload.get("prompt")
        params = imported_state.get("params", {})

        if not imported_state or not user_prompt:
            return JSONResponse(content={"error": "Invalid payload. 'imported_state' and 'prompt' are required."}, status_code=400)

        if params.get("coder_debug_mode") == 'true':
            llm = CoderMockLLM()
        elif params.get("debug_mode") == 'true':
            llm = MockLLM()
        else:
            model_name = params.get("ollama_model", "dengcao/Qwen3-3B-A3B-Instruct-2507:latest")
            llm = ChatOllama(model=model_name, temperature=0.8)

        imported_state["original_request"] = user_prompt
        imported_state["current_problem"] = user_prompt
        imported_state["agent_outputs"] = {} 

        workflow = StateGraph(GraphState)
        all_layers_prompts = imported_state["all_layers_prompts"]
        cot_trace_depth = len(all_layers_prompts)

        agent_chain = ChatPromptTemplate.from_template("{input}") | llm | StrOutputParser()

        async def inference_agent_logic(state: GraphState, node_id: str):
            await log_stream.put(f"--- [INFERENCE] Invoking Agent: {node_id} ---")
            layer_index_str, agent_index_str = node_id.split('_')[1:]
            layer_index = int(layer_index_str)
            agent_prompt = state['all_layers_prompts'][layer_index][int(agent_index_str)]

            if layer_index == 0:
                input_data = state["original_request"]
            else:
                prev_layer_index = layer_index - 1
                num_agents_prev_layer = len(state['all_layers_prompts'][prev_layer_index])
                prev_layer_outputs = [state["agent_outputs"].get(f"agent_{prev_layer_index}_{k}", {}) for k in range(num_agents_prev_layer)]
                input_data = json.dumps(prev_layer_outputs)

            full_prompt = f"{agent_prompt}\n\nInput Data to Process:\n---\n{input_data}\n---\nYour JSON formatted response:"
            response_str = await agent_chain.ainvoke({"input": full_prompt})

            try:
                response_json = clean_and_parse_json(response_str)
            except Exception:
                response_json = {"proposed_solution": response_str, "reasoning": "Inference output could not be parsed as JSON."}

            current_outputs = state.get("agent_outputs", {}).copy()
            current_outputs[node_id] = response_json
            return {"agent_outputs": current_outputs}

        def create_inference_node_function(node_id_for_closure: str):
            async def node_function(state: GraphState):
                return await inference_agent_logic(state, node_id_for_closure)
            return node_function

        for i, layer_prompts in enumerate(all_layers_prompts):
            for j, _ in enumerate(layer_prompts):
                node_id = f"agent_{i}_{j}"
                workflow.add_node(node_id, create_inference_node_function(node_id))

        workflow.add_node("synthesis", create_synthesis_node(llm))

        first_layer_nodes = [f"agent_0_{j}" for j in range(len(all_layers_prompts[0]))]
        workflow.set_entry_point(first_layer_nodes[0])
        if len(first_layer_nodes) > 1:

            for node in first_layer_nodes[1:]:
                 workflow.add_edge(first_layer_nodes[0], node)

        for i in range(cot_trace_depth - 1):
            for current_node in [f"agent_{i}_{j}" for j in range(len(all_layers_prompts[i]))]:
                for next_node in [f"agent_{i+1}_{k}" for k in range(len(all_layers_prompts[i+1]))]:
                    workflow.add_edge(current_node, next_node)

        for node in [f"agent_{cot_trace_depth-1}_{j}" for j in range(len(all_layers_prompts[cot_trace_depth-1]))]:
            workflow.add_edge(node, "synthesis")

        workflow.add_edge("synthesis", END)
        graph = workflow.compile()

        ascii_diagram = graph.get_graph().draw_ascii()
        await log_stream.put(ascii_diagram)


        final_result_node = None
        async for output in graph.astream(imported_state):
            if "synthesis" in output:
                final_result_node = output["synthesis"]

        await log_stream.put("--- [INFERENCE-ONLY] Run complete. ---")

        return JSONResponse(content={
            "message": "Inference complete.",
            "code_solution": final_result_node.get("final_solution", {}).get("proposed_solution", "No solution generated."),
            "reasoning": final_result_node.get("final_solution", {}).get("reasoning", "No reasoning provided."),
            "is_inference": True
        })

    except Exception as e:
        error_message = f"An error occurred during inference: {e}"
        await log_stream.put(error_message)
        await log_stream.put(traceback.format_exc())
        return JSONResponse(content={"message": error_message, "traceback": traceback.format_exc()}, status_code=500)




@app.post("/build_and_run_graph")
async def build_and_run_graph(payload: dict = Body(...)):
    llm = None
    embeddings_model = None
    summarizer_llm = None
    params = payload.get("params", {})
    mode = payload.get("mode", "algorithm")
    
    # Initialize Token Tracker
    token_tracker = TokenUsageTracker(log_stream)

    try:
        # Determine Provider
        provider = params.get("provider", "ollama")
        api_key = params.get("api_key", "")
        
        # Custom Debug Mode Logic (Prioritize Mock LLMs)
        if params.get("coder_debug_mode") == 'true':
            await log_stream.put(f"--- ðŸ’» CODER DEBUG MODE ENABLED ðŸ’» ---")
            llm = CoderMockLLM()
            summarizer_llm = CoderMockLLM()
            embeddings_model = OllamaEmbeddings(model="mxbai-embed-large:latest")
            
        elif params.get("debug_mode") == 'true':
            await log_stream.put(f"--- ðŸš€ DEBUG MODE ENABLED ðŸš€ ---")
            llm = MockLLM()
            summarizer_llm = MockLLM()
            embeddings_model = OllamaEmbeddings(model="mxbai-embed-large:latest")

        elif provider == "gemini":
            if not api_key:
                return JSONResponse(content={"message": "Gemini API Key required"}, status_code=400)
            llm = ChatGoogleGenerativeAI(model="gemini-3-flash-preview", google_api_key=api_key, temperature=0.7, callbacks=[token_tracker])
            summarizer_llm = llm # Reuse for summary
            embeddings_model = OllamaEmbeddings(model="mxbai-embed-large:latest") 
            await log_stream.put(f"--- Initializing Main Agent LLM: Gemini (gemini-3-flash-preview) ---")
            
        elif provider == "grok":
            if not api_key:
                return JSONResponse(content={"message": "Grok API Key required"}, status_code=400)
            llm = ChatXAI(model="grok-4-1-fast", xai_api_key=api_key, temperature=0.7, callbacks=[token_tracker])
            summarizer_llm = llm
            embeddings_model = OllamaEmbeddings(model="mxbai-embed-large:latest")
            await log_stream.put(f"--- Initializing Main Agent LLM: Grok (grok-4.1 fast) ---")

        else:
            # Default Ollama
            summarizer_llm = ChatOllama(model="qwen3:1.7b", temperature=0, callbacks=[token_tracker])
            embeddings_model = OllamaEmbeddings(model="mxbai-embed-large:latest")
            model_name = params.get("ollama_model", "dengcao/Qwen3-3B-A3B-Instruct-2507:latest")
            await log_stream.put(f"--- Initializing Main Agent LLM: Ollama ({model_name}) ---")

            llm = ChatOllama(model=model_name, temperature=0.4, callbacks=[token_tracker])
            await llm.ainvoke("Hi")
            await log_stream.put("--- Main Agent LLM Connection Successful ---")

    except Exception as e:
        error_message = f"Failed to initialize LLM: {e}. Please ensure the selected provider is configured correctly."
        await log_stream.put(error_message)
        return JSONResponse(content={"message": error_message, "traceback": traceback.format_exc()}, status_code=500)
    
    user_prompt = params.get("prompt")
    code_detection_chain = get_code_detector_chain(llm)
    is_code = (await code_detection_chain.ainvoke({"text": user_prompt})).strip().lower() == 'true'
    
    if mode == "brainstorm":
        is_code = False # Force false for brainstorming

    await log_stream.put(f"--- Starting Graph Build and Run Process (Mode: {mode}) ---")
    await log_stream.put(f"Parameters: {params}")

    decomposed_problems_map = {}
    all_layers_prompts = []
    agent_personas = {}
    
    try:
        if mode == "brainstorm":
            # BRAINSTORM MODE SETUP (Dynamic Spanning)
            await log_stream.put("--- [BRAINSTORM] Analyzing Complexity & Spanning Concept Space ---")
            
            # Extract chat history and document context from payload
            chat_history = payload.get("chat_history", [])
            document_context = payload.get("document_context", "")
            
            # Format chat history as string for context
            chat_history_str = ""
            if chat_history:
                chat_history_str = "\n".join([
                    f"{'User' if msg.get('role') == 'user' else 'Assistant'}: {msg.get('content', '')}"
                    for msg in chat_history
                ])
                await log_stream.put(f"LOG: Chat history contains {len(chat_history)} messages.")
            
            if document_context:
                await log_stream.put(f"LOG: Document context provided ({len(document_context)} characters).")
            
            # 1. Complexity Estimation
            complexity_chain = get_complexity_estimator_chain(llm)
            complexity_result_str = await complexity_chain.ainvoke({
                "user_input": user_prompt,
                "prior_conversation": chat_history_str,
                "document_context": document_context[:10000] if document_context else ""  # Truncate for estimation
            })

            # --- Problem Summarization (if documents are present) ---
            brainstorm_problem_summary = ""
            if document_context:
                await log_stream.put("LOG: [BRAINSTORM] Summarizing problem and documents for agent context...")
                summarizer_chain = get_problem_summarizer_chain(llm) # Use main LLM for summarization
                brainstorm_problem_summary = await summarizer_chain.ainvoke({
                    "user_input": user_prompt,
                    "document_context": document_context[:50000] # Limit for summarization context window
                })
                # await log_stream.put(f"LOG: [BRAINSTORM] Summary generated.")

            # --- Problem Summarization (if documents are present) ---
            brainstorm_problem_summary = ""
            if document_context:
                await log_stream.put("LOG: [BRAINSTORM] Summarizing problem and documents for agent context...")
                summarizer_chain = get_problem_summarizer_chain(llm) # Use main LLM for summarization
                brainstorm_problem_summary = await summarizer_chain.ainvoke({
                    "user_input": user_prompt,
                    "document_context": document_context[:50000] # Limit for summarization context window
                })
                # await log_stream.put(f"LOG: [BRAINSTORM] Summary generated.")
            
            width = 3 # Default
            
            try:
                complexity_data = clean_and_parse_json(complexity_result_str)
                # User determines epochs (default 2), Complexity determines Topology
                if 'num_epochs' not in params:
                     params['num_epochs'] = 2
                else:
                     params['num_epochs'] = int(params['num_epochs'])
                     
                cot_trace_depth = int(complexity_data.get("recommended_layers", 2))
                width = int(complexity_data.get("recommended_width", 3))
                
                await log_stream.put(f"LOG: Topology: {cot_trace_depth} Layers x {width} Width x {params['num_epochs']} Epochs.")
            except Exception as e:
                await log_stream.put(f"WARNING: Complexity estimation failed. Using defaults. Error: {e}")
                if 'num_epochs' not in params: params['num_epochs'] = 2
                else: params['num_epochs'] = int(params['num_epochs'])
                cot_trace_depth = 2
                width = 3

            # 2. Seed Generation (Guiding Concepts)
            # Generate distinct concepts to span the problem space
            seed_chain = get_brainstorming_seed_chain(llm)
            concepts_str = await seed_chain.ainvoke({"problem": user_prompt, "num_concepts": width})
            guiding_concepts = [c.strip() for c in concepts_str.split() if c.strip()]
            
            # Ensure we have enough concepts
            while len(guiding_concepts) < width:
                guiding_concepts.append("General_Analysis")
            guiding_concepts = guiding_concepts[:width]
            
            await log_stream.put(f"LOG: Guiding Concepts: {', '.join(guiding_concepts)}")

            # 3. Dynamic Spanning (Persona Generation)
            spanner_chain = get_brainstorming_spanner_chain(llm)
            
            for i in range(cot_trace_depth):
                layer_prompts = []
                for j in range(width):
                    agent_id = f"agent_{i}_{j}"
                    concept = guiding_concepts[j % len(guiding_concepts)]
                    
                    # Generate Unique Node Persona
                    # await log_stream.put(f"LOG: Generating Persona for Layer {i}, Node {j} (Focus: {concept})...") 
                    # Commented out verbose logging to reduce noise, but process is happening
                    
                    persona_str = await spanner_chain.ainvoke({
                        "problem": user_prompt,
                        "guiding_concept": concept,
                        "layer_index": i, 
                        "node_index": j,
                        "document_context": document_context[:5000] if document_context else ""  # Truncate for persona generation
                    })
                    
                    try:
                        persona = clean_and_parse_json(persona_str)
                    except:
                        # Fallback
                        persona = {
                            "name": f"Expert {i}-{j}", 
                            "specialty": f"{concept} Specialist" if concept != "General_Analysis" else "Analyst", 
                            "emoji": "ðŸ§ ",
                            "system_prompt": f"You are an expert in {concept}. Analyze the topic: {user_prompt}."
                        }
                    
                    system_prompt = f"""
You are {persona.get('name', 'Expert')} {persona.get('emoji', 'ðŸ§ ')}.
Your Specialty is: {persona.get('specialty', 'Analysis')}.

<Role>
{persona.get('system_prompt', 'Analyze the input.')}
</Role>
"""
                    layer_prompts.append(system_prompt)
                    
                    # Persist metadata
                    agent_personas[agent_id] = {
                        "name": persona.get('name', f"Agent {i}-{j}"),
                        "mbti_type": "Expert", 
                        "specialty": persona.get('specialty', 'Analysis')
                    }
                    decomposed_problems_map[agent_id] = user_prompt 
                
                all_layers_prompts.append(layer_prompts)
            
            await log_stream.put(f"LOG: Successfully generated {len(all_layers_prompts) * width} unique expert personas.")

                
        else:
             # ALGORITHM MODE SETUP (Existing Logic)
            mbti_archetypes = params.get("mbti_archetypes")
            word_vector_size = int(params.get("vector_word_size"))
            cot_trace_depth = int(params.get('cot_trace_depth', 3))

            if not mbti_archetypes or len(mbti_archetypes) < 2:
                 return JSONResponse(content={"message": "Select at least 2 MBTI archetypes."}, status_code=400)

            await log_stream.put("--- Decomposing Original Problem into Subproblems ---")
            num_agents_per_layer = len(mbti_archetypes)
            total_agents_to_create = num_agents_per_layer * cot_trace_depth
            decomposition_chain = get_problem_decomposition_chain(llm)
            
            sub_problems_str = await decomposition_chain.ainvoke({
                "problem": user_prompt,
                "num_sub_problems": total_agents_to_create
            })
            try:
                sub_problems_list = clean_and_parse_json(sub_problems_str).get("sub_problems", [])
            except:
                sub_problems_list = [user_prompt] * total_agents_to_create

            problem_idx = 0
            for i in range(cot_trace_depth):
                for j in range(num_agents_per_layer):
                    agent_id = f"agent_{i}_{j}"
                    if problem_idx < len(sub_problems_list):
                        decomposed_problems_map[agent_id] = sub_problems_list[problem_idx]
                        problem_idx += 1
                    else:
                        decomposed_problems_map[agent_id] = user_prompt

            # Generate Seeds & Spanners
            all_verbs = [] # ... skipped full seed logic re-implementation for brevity, relying on user request to refactor, assuming simple copy valid or just condensed
            # ACTUALLY I MUST KEEP THE LOGIC.
            # I will assume the original logic was:
            num_mbti_types = len(mbti_archetypes)
            total_verbs_to_generate = word_vector_size * num_mbti_types
            seed_generation_chain = get_seed_generation_chain(llm)
            generated_verbs_str = await seed_generation_chain.ainvoke({"problem": user_prompt, "word_count": total_verbs_to_generate})
            all_verbs = list(set(generated_verbs_str.split()))
            random.shuffle(all_verbs)
            seeds = {mbti: " ".join(random.sample(all_verbs, word_vector_size)) for mbti in mbti_archetypes}
            
            input_spanner_chain = get_input_spanner_chain(llm, params['prompt_alignment'], params['density'])
            
            for i in range(cot_trace_depth):
                layer_prompts = []
                for j, (m, gw) in enumerate(seeds.items()):
                     agent_id = f"agent_{i}_{j}"
                     # Generate prompt...
                     prompt = await input_spanner_chain.ainvoke({
                        "mbti_type": m, "guiding_words": gw, 
                        "sub_problem": decomposed_problems_map[agent_id],
                        "critique": "", "name": names.get_full_name()
                     })
                     layer_prompts.append(prompt)
                     agent_personas[agent_id] = {"name": names.get_full_name(), "mbti_type": m}
                all_layers_prompts.append(layer_prompts)

    except Exception as e:
         error_message = f"Error during graph setup: {e}"
         await log_stream.put(error_message)
         await log_stream.put(traceback.format_exc())
         return JSONResponse(content={"message": error_message}, status_code=500)

    # Building Graph Nodes
    workflow = StateGraph(GraphState)
    
    # Add Nodes
    for i, layer_prompts in enumerate(all_layers_prompts):
        for j, _ in enumerate(layer_prompts):
            node_id = f"agent_{i}_{j}"
            workflow.add_node(node_id, create_agent_node(llm, node_id))
    
    workflow.add_node("synthesis", create_synthesis_node(llm))
    workflow.add_node("code_execution", create_code_execution_node(llm))
    workflow.add_node("archive_epoch", create_archive_epoch_outputs_node())
    workflow.add_node("metrics", create_metrics_node(llm))
    workflow.add_node("reframe_and_decompose", create_reframe_and_decompose_node(llm))
    workflow.add_node("update_prompts", create_update_agent_prompts_node(llm))

    # Add Edges (Architecture)
    # Layer 0 -> Layer 1 ... -> Synthesis
    
    first_layer_nodes = [f"agent_0_{j}" for j in range(len(all_layers_prompts[0]))]
    first_layer_nodes = [f"agent_0_{j}" for j in range(len(all_layers_prompts[0]))]
    
    # Parallel Entry: Connect START to ALL Layer 0 nodes
    for n in first_layer_nodes:
        workflow.add_edge(START, n)

    
    for i in range(len(all_layers_prompts) - 1):
        current_layer_nodes = [f"agent_{i}_{j}" for j in range(len(all_layers_prompts[i]))]
        next_layer_nodes = [f"agent_{i+1}_{k}" for k in range(len(all_layers_prompts[i+1]))]
        for curr in current_layer_nodes:
            for nxt in next_layer_nodes:
                workflow.add_edge(curr, nxt)
    
    last_layer_nodes = [f"agent_{len(all_layers_prompts)-1}_{j}" for j in range(len(all_layers_prompts[-1]))]
    for n in last_layer_nodes:
        workflow.add_edge(n, "synthesis")
        
    workflow.add_edge("synthesis", "code_execution")
    workflow.add_edge("code_execution", "archive_epoch")
    workflow.add_edge("archive_epoch", "metrics")
    
    # Conditional Edge for Loops
    def epoch_gateway(state):
        if state["epoch"] < state["max_epochs"]:
             return "reframe_and_decompose"
        return "harvest"

    workflow.add_conditional_edges(
        "metrics",
        epoch_gateway,
        {
            "reframe_and_decompose": "reframe_and_decompose",
            "harvest": END 
        }
    )
    # Note: Using END here effectively means we break the loop if done.
    # But wait, we need to add Harvest Node to graph if we link to it? 
    # Or handled by app logic?
    # Original logic had "harvest" key mapping to... END?
    # Actually, harvest is usually a separate call or node.
    # We should map "harvest" to END, and let the frontend call /harvest if needed?
    # OR create a harvest node?
    # The original code had conditional edge to "reframe..." or END.
    
    workflow.add_edge("reframe_and_decompose", "update_prompts")
    # Loop back to Entry Point is tricky with LangGraph.
    # We need to restart the agent nodes.
    # Connect update_prompts to Layer 0 nodes?
    for n in first_layer_nodes:
        workflow.add_edge("update_prompts", n)

    graph = workflow.compile()
    
    ascii_art = graph.get_graph().draw_ascii()
    await log_stream.put(ascii_art)
    
    session_id = str(uuid.uuid4())
    
    # Prepare brainstorm context (only relevant in brainstorm mode, but always include empty defaults)
    brainstorm_chat_history_str = ""
    brainstorm_document_context = ""
    if mode == "brainstorm":
        brainstorm_chat_history_str = chat_history_str
        brainstorm_document_context = document_context
    
    initial_state = {
        "session_id": session_id,
        "mode": mode,
        "original_request": user_prompt,
        "current_problem": user_prompt,
        "decomposed_problems": decomposed_problems_map,
        "epoch": 0,
        "max_epochs": int(params.get("num_epochs", 1)),
        "params": params, 
        "all_layers_prompts": all_layers_prompts,
        "agent_personas": agent_personas,
        "is_code_request": is_code, 
        "agent_outputs": {}, 
        "memory": {}, 
        "final_solution": None,
        "previous_solution": "",
        "chat_history": [],
        "layers": [], 
        "critiques": {}, 
        "perplexity_history": [],
        "raptor_index": None,
        "all_rag_documents": [],
        "academic_papers": None, 
        "summarizer_llm": summarizer_llm,
        "embeddings_model": embeddings_model, 
        "modules": [],
        "synthesis_context_queue": [],
        "synthesis_execution_success": True,
        # Brainstorm mode context
        "brainstorm_prior_conversation": brainstorm_chat_history_str,
        "brainstorm_document_context": brainstorm_document_context
    }
    initial_state["llm"] = llm
    sessions[session_id] = initial_state
    
    await log_stream.put(f"__session_id__ {session_id}")
    await log_stream.put(f"__start__ {ascii_art}") # Send start signal + ASCII
    
    # Run Graph
    asyncio.create_task(run_graph_background(graph, initial_state))

    return JSONResponse(content={
        "message": "Graph started.",
        "session_id": session_id
    })

async def run_graph_background(graph, initial_state):
    session_id = initial_state["session_id"]
    try:
         async for output in graph.astream(initial_state, {'recursion_limit': 100}):
            for node_name, node_output in output.items():
                if node_output:
                     # Update session state
                     current = sessions[session_id]
                     for k,v in node_output.items():
                         if isinstance(current.get(k), dict) and isinstance(v, dict):
                             current[k].update(v)
                         elif isinstance(current.get(k), list) and isinstance(v, list):
                             current[k].extend(v)
                         else:
                             current[k] = v
                     sessions[session_id] = current
    except Exception as e:
        await log_stream.put(f"Graph Background Error: {e}")

@app.get("/export_qnn/{session_id}")
async def export_qnn(session_id: str):
    """
    Exports the current state of a session graph to a JSON file.
    """
    if session_id not in sessions:
        return JSONResponse(content={"error": "Session not found."}, status_code=404)

    state_to_export = sessions[session_id].copy()


    state_to_export.pop('llm', None)
    state_to_export.pop('summarizer_llm', None)
    state_to_export.pop('embeddings_model', None)
    state_to_export.pop('raptor_index', None) 

    for idx, document in enumerate(state_to_export['all_rag_documents']):
        state_to_export['all_rag_documents'][idx] = document.dict()
    

    await log_stream.put(f"--- [EXPORT] Exporting QNN for session {session_id} ---")

    return JSONResponse(
        content=state_to_export,
        headers={"Content-Disposition": f"attachment; filename=qnn_state_{session_id}.json"}
    )

@app.post("/import_qnn")
async def import_qnn(file: UploadFile = File(...)):
    """
    Imports a QNN JSON file to initialize a new session.
    """
    try:
        content = await file.read()
        imported_state = json.loads(content)
        
        session_id = str(uuid.uuid4())
        imported_state['session_id'] = session_id

        for idx, document in enumerate(imported_state['all_rag_documents']): 

            imported_state['all_rag_documents'][idx] = Document.from_dict(document)

        sessions[session_id] = imported_state
        await log_stream.put(f"--- [IMPORT] Successfully imported QNN file. New Session ID: {session_id} ---")
        
        return JSONResponse(content={
            "message": "QNN file imported successfully.",
            "session_id": session_id,
            "imported_params": imported_state.get("params", {})
        })
    except Exception as e:
        error_message = f"Failed to import QNN file: {e}"
        await log_stream.put(error_message)
        return JSONResponse(content={"message": error_message, "traceback": traceback.format_exc()}, status_code=500)


@app.post("/upload_documents")
async def upload_documents(files: List[UploadFile] = File(...)):
    """
    Uploads PDF documents and extracts their text content.
    Returns extracted text to be used as context in brainstorm mode.
    """
    MAX_TOTAL_CHARS = 50000  # Limit to prevent token overflow
    
    extracted_texts = []
    total_chars = 0
    
    try:
        for file in files:
            if not file.filename.lower().endswith('.pdf'):
                await log_stream.put(f"WARNING: Skipping non-PDF file: {file.filename}")
                continue
            
            content = await file.read()
            
            # Use PyMuPDF to extract text
            try:
                pdf_document = fitz.open(stream=content, filetype="pdf")
                file_text = ""
                
                for page_num in range(len(pdf_document)):
                    page = pdf_document[page_num]
                    file_text += page.get_text()
                
                pdf_document.close()
                
                # Truncate if needed
                remaining_chars = MAX_TOTAL_CHARS - total_chars
                if remaining_chars <= 0:
                    await log_stream.put(f"WARNING: Character limit reached. Skipping remaining files.")
                    break
                
                if len(file_text) > remaining_chars:
                    file_text = file_text[:remaining_chars]
                    await log_stream.put(f"WARNING: Truncated {file.filename} to fit character limit.")
                
                total_chars += len(file_text)
                extracted_texts.append({
                    "filename": file.filename,
                    "text": file_text,
                    "char_count": len(file_text)
                })
                
                await log_stream.put(f"SUCCESS: Extracted {len(file_text)} characters from {file.filename}")
                
            except Exception as pdf_error:
                await log_stream.put(f"ERROR: Failed to extract text from {file.filename}: {pdf_error}")
                continue
        
        # Combine all extracted texts
        combined_text = "\n\n---\n\n".join([
            f"[Document: {doc['filename']}]\n{doc['text']}" 
            for doc in extracted_texts
        ])
        
        return JSONResponse(content={
            "message": f"Successfully extracted text from {len(extracted_texts)} document(s).",
            "documents": extracted_texts,
            "combined_text": combined_text,
            "total_chars": total_chars
        })
        
    except Exception as e:
        error_message = f"Failed to process documents: {e}"
        await log_stream.put(error_message)
        return JSONResponse(content={"message": error_message, "traceback": traceback.format_exc()}, status_code=500)


@app.post("/chat")
async def chat_with_index(payload: dict = Body(...)):
    message = payload.get("message")
    session_id = payload.get("session_id") 

    print("Sesion keys: ", sessions.keys())
    print("Session ID: ", session_id)
 
    if not session_id or session_id not in list(sessions.keys()):
        return JSONResponse(content={"error": "Invalid session ID"}, status_code=404)

    state = sessions[session_id]

    raptor_index = state.get("raptor_index")
    llm = state["llm"]

    if not raptor_index:
        return JSONResponse(content={"error": "RAG index not found for this session"}, status_code=500)

    async def stream_response():
        try:
            retrieved_docs = await asyncio.to_thread(raptor_index.retrieve, message, k=10)
            context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])

            chat_chain = get_rag_chat_chain(llm)
            full_response = ""
            async for chunk in chat_chain.astream({"context": context, "question": message}):
                content = chunk.content if hasattr(chunk, 'content') else chunk
                yield content
                full_response += content

            state["chat_history"].append({"role": "user", "content": message})
            state["chat_history"].append({"role": "ai", "content": full_response})

        except Exception as e:

            print(f"Error during chat streaming: {e}")
            yield f"Error: Could not generate response. {e}"

    return StreamingResponse(stream_response(), media_type="text/event-stream")

@app.post("/diagnostic_chat")
async def diagnostic_chat_with_index(payload: dict = Body(...)):
    message = payload.get("message")
    session_id = payload.get("session_id")
    message = payload.get("message")

    print("Sesion keys: ", sessions.keys())
    print("Session ID: ", session_id)

    if not session_id or session_id not in list(sessions.keys()):
        return JSONResponse(content={"error": "Invalid session ID"}, status_code=404)

    print("Entering diagnostic_chat_with_index")

    state = sessions[session_id]
    raptor_index = state.get("raptor_index")

    if not raptor_index:
        async def stream_error():
            yield "The RAG index for this session is not yet available. Please wait for the first epoch to complete."
        return StreamingResponse(stream_error(), media_type="text/event-stream")
        
    async def stream_response():
        try:
            query = message.strip()[5:]
            await log_stream.put(f"--- [DIAGNOSTIC] Raw RAG query received: '{query}' ---")
                
            retrieved_docs = await asyncio.to_thread(raptor_index.retrieve, query, k=10)
                
            if not retrieved_docs:
                yield "No relevant documents found in the RAPTOR index for that query."
                return

            yield "--- Top Relevant Documents (Raw Retrieval) ---\n\n"
            for i, doc in enumerate(retrieved_docs):
                    content_preview = doc.page_content.replace('\n', ' ').strip()
                    metadata_str = json.dumps(doc.metadata)
                    response_chunk = (
                        f"DOCUMENT #{i+1}\n"
                        f"-----------------\n"
                        f"METADATA: {metadata_str}\n"
                        f"CONTENT: {content_preview}...\n\n"
                    )
                    yield response_chunk

        except Exception as e:
            print(f"Error during diagnostic chat streaming: {e}")
            yield f"Error: Could not generate response. {e}"

    return StreamingResponse(stream_response(), media_type="text/event-stream")

@app.post("/harvest")
async def harvest_session(payload: dict = Body(...)):


    if not payload.get("session_id") or payload.get("session_id") not in list(sessions.keys()):
        return JSONResponse(content={"error": "Invalid request"}, status_code=404)

    session =  sessions.get(payload.get("session_id"))

    if not session:
        return JSONResponse(content={"error": "Invalid request"}, status_code=404)

    try:
        await log_stream.put("--- [HARVEST] Initiating Final Harvest Process ---")
        state = session 
        chat_history = session["chat_history"]
        llm = session["llm"]
        summarizer_llm = session["summarizer_llm"]
        embeddings_model = session["embeddings_model"]
        params = session["params"]

        chat_docs = []
        if chat_history:
            for i, turn in enumerate(chat_history):
                 if turn['role'] == 'ai':
                    user_turn = chat_history[i-1]
                    content = f"User Question: {user_turn['content']}\n\nAI Answer: {turn['content']}"
                    chat_docs.append(Document(page_content=content, metadata={"source": "chat_session", "turn": i//2}))
            await log_stream.put(f"LOG: Converted {len(chat_history)} chat turns into {len(chat_docs)} documents.")
            state["all_rag_documents"].extend(chat_docs)
            await log_stream.put(f"LOG: Added chat documents. Total RAG documents now: {len(state['all_rag_documents'])}.")
            
            await log_stream.put("--- [RAG PASS] Re-building Final RAPTOR Index with Chat History ---")
            update_rag_node = create_update_rag_index_node(summarizer_llm, embeddings_model)
            update_result = await update_rag_node(state, end_of_run=True)
            state.update(update_result)

        num_questions = int(params.get('num_questions', 25))
        final_harvest_node = create_final_harvest_node(llm, summarizer_llm, num_questions)
        final_harvest_result = await final_harvest_node(state)
        state.update(final_harvest_result)

        academic_papers = state.get("academic_papers", {})
        session_id = state.get("session_id", "")

        if academic_papers:
            final_reports[session_id] = academic_papers
            await log_stream.put(f"SUCCESS: Final report with {len(academic_papers)} papers created.")
        else:
            await log_stream.put("WARNING: No academic papers were generated in the final harvest.")


        return JSONResponse(content={
            "message": "Harvest complete.",
        })

    except Exception as e:
        error_message = f"An error occurred during harvest: {e}"
        await log_stream.put(error_message)
        await log_stream.put(traceback.format_exc())
        return JSONResponse(content={"message": error_message, "traceback": traceback.format_exc()}, status_code=500)


@app.get('/stream_log')
async def stream_log(request: Request):

    async def event_generator():
        while True:
            if await request.is_disconnected():
                print("Client disconnected from log stream.")
                break
            try:
                log_message = await asyncio.wait_for(log_stream.get(), timeout=1.0)
                yield f"data: {log_message}\n\n"
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                print(f"Error in stream: {e}")
                break

    return EventSourceResponse(event_generator())


@app.get("/download_report/{session_id}")
async def download_report(session_id: str):


    papers = final_reports.get(session_id, {})

    if not papers:
        return JSONResponse(content={"error": "Report not found or expired."}, status_code=404)

    zip_buffer = io.BytesIO()
    with zipfile.ZipFile(zip_buffer, "a", zipfile.ZIP_DEFLATED, False) as zip_file:
        for i, (question, content) in enumerate(papers.items()):
            safe_question = re.sub(r'[^\w\s-]', '', question).strip().replace(' ', '_')
            filename = f"paper_{i+1}_{safe_question[:50]}.md"
            zip_file.writestr(filename, content)

    zip_buffer.seek(0)

    return StreamingResponse(
        zip_buffer,
        media_type="application/zip",
        headers={"Content-Disposition": f"attachment; filename=NOA_Report_{session_id}.zip"}
    )


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)